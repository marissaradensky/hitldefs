{"doc_id": 0,"title": "sgc","abstract": ["We refer to this simplified linear model as Simple Graph Convolution (SGC).","\nIn contrast to its nonlinear counterparts, the SGC is intuitively interpretable and we provide a theoretical analysis from the graph convolution perspective.","Notably, feature extraction in SGC corresponds to a single fixed filter applied to each feature dimension.","\n07 15\n3v 1\n[ cs\n.L G\n] 1\n9 Fe\nb 20\n19\nSGC.","\nThrough an empirical assessment on node classification benchmark datasets for citation and social networks, we show that the SGC achieves comparable performance to GCN and other state-of-the-art graph neural networks.","Finally, we demonstrate that SGC extrapolates its effectiveness to a wide-range of downstream tasks.","In particular, SGC rivals, if not surpasses, GCN-based approaches on text classification, user geolocation, relation extraction, and zero-shot image classification tasks.","\nWe follow Kipf & Welling (2017) to introduce GCNs (and subsequently SGC) in the context of node classification.","We define the degree matrix D = diag(d1, . . . , dn) as a\n1https://github.com/Tiiiger/SGC\ndiagonal matrix where each entry on the diagonal is equal to the row-sum of the adjacency matrix di = \u2211 j aij .","The resulting classifier becomes\nY\u0302SGC = softmax ( SKX\u0398 ) , (8)\nwhich we refer to as Simple Graph Convolution (SGC)."]}
{"doc_id": 1,"title": "shap","abstract": ["We then prove that results from game theory that guarantee a unique solution apply to the entire class of additive feature attribution methods (Section 3), and we propose SHAP values as a unified measure of feature importance that other methods approximate (Section 4).","We propose new SHAP value estimation methods, and demonstrate they show better agreement with human intuition as measured by user studies and better discriminate between model output classes than existing methods (Section 5).","\nClassic Shapley value estimation: There are three previous methods that use classic equations from cooperative game theory to compute explanations of model predictions: Shapley regression values [3], Shapley sampling values [8], and Quantitative Input Influence [2].","\nShapley regression values is a method designed to compute feature importance for linear models in the presence of multicollinearity.","Then the prediction of the two models is compared on the current input fS\u222a{i}(xS\u222a{i})\u2212 fS(xS), where xS represents the values of the input features in the set S. Since the effect of withholding a feature depends on what other features are present in the model, the above differences are computed for all possible subsets S \u2286 F \\ {i}. The Shapley values, an allocation method from cooperative game theory, are then computed and used as the feature attributions.","The Shapley values are a weighted average of all possible differences:\n\u03c6i = \u2211\nS\u2286F\\{i}\n|S|!(|F | \u2212 |S| \u2212 1)! |F |!\n[ fS\u222a{i}(xS\u222a{i})\u2212 fS(xS) ] .","(4)\nFor Shapley regression values the mapping hx takes the original input and maps it to 1 or 0, where 1 means the input is included in the model, while 0 means it is excluded from the model. If we let \u03c60 = f\u2205(\u2205) then the Shapley regression values match Equation 1, and are hence an additive feature attribution method.","\nShapley sampling values are meant to explain any model by applying sampling approximations to Equation 4, and by approximating the effect of removing a variable from the model by integrating over samples from the training data set.","Since the explanation model form of Shapley sampling values is the same as Shapley regression values it is also an additive feature attribution method.","\nQuantitative Input Influence is a broader framework that addresses more than just feature attributions, but as part of their method they independently proposed a sampling approximation to Shapley values that is nearly identical to Shapley sampling values."]}
{"doc_id": 2,"title": "cbow","abstract": ["(4) We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context.","\nThe second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence.","The results using the CBOW architecture with different choice of word vector dimensionality and increasing amount of the training data are shown in Table 2.","The CBOW architecture works better than the NNLM on the syntactic tasks, and about the same on the semantic one.","Finally, the Skip-gram architecture works slightly worse on the syntactic task than the CBOW model (but still better than the NNLM), and much better on the semantic part of the test than all the other models.","The CBOW model was trained on subset\n3We thank Geoff Zweig for providing us the test set.","Note that due to the overhead of the distributed framework, the CPU usage of the CBOW model and the Skip-gram model are much closer to each other than their single-machine implementations.","Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary."]}
{"doc_id": 3,"title": "xgboost","abstract": [ "\nIn this paper, we describe XGBoost, a scalable machine learning system for tree boosting.", "Among the 29 challenge winning solutions 3 published at Kaggle\u2019s blog during 2015, 17 solutions used XGBoost.", "Among these solutions, eight solely used XGBoost to train the model, while most others combined XGBoost with neural nets in ensembles.", "The success of the system was also witnessed in KDDCup 2015, where XGBoost was used by every winning team in the top10.", "Moreover, the winning teams reported that ensemble methods outperform a well-configured XGBoost by only a small amount [1].", "While domain dependent data analysis and feature engineering play an important role in these solutions, the fact that XGBoost is the consensus choice of learner shows the impact and importance of our system and tree boosting.", "\nThe most important factor behind the success of XGBoost is its scalability in all scenarios.", "The scalability of XGBoost is due to several important systems and algorithmic optimizations.", "More importantly, XGBoost exploits out-of-core\n2https://github.com/dmlc/xgboost 3Solutions come from of top-3 teams of each competitions.", "Most existing single machine tree boosting implementations, such as scikit-learn [20], R\u2019s gbm [21] as well as the single machine version of XGBoost support the exact greedy algorithm."]}
{"doc_id": 4,"title": "hnsw","abstract": [ "controllable hierarchy (Hierarchical NSW, HNSW).", "\nIn this paper we propose the Hierarchical Navigable Small World (Hierarchical NSW, HNSW), a new fully graph based incremental K-ANNS structure, which can offer a much better logarithmic complexity scaling.", "INSERT(hnsw; q, M,Mmax, efConstruction, mL)\nInput: multilayer graph hnsw, new element q, number of established connections M, maximum number of connections for each element per layer Mmax, size of the dynamic candidate list efConstruction, normalization factor for level generationmL Output: update hnsw inserting element q 1 W ;== list for the currently found nearest elements 2 ep get enter-point for hnsw 3 L level of ep // top layer for hnsw 4 l b ln\u00f0unif\u00f00::1\u00de\u00de mLc // new element\u2019s level 5 for lc L . . .", "0 9 W SEARCH-LAYER(q, ep, efConstruction, lc) 10 neighbors SELECT-NEIGHBORS(q,W,M, lc) // Algorithm 3 or Algorithm 4 11 add bidirectionall connectionts from neighbors to q at layer lc 12 for each e 2 neighbors // shrink connections if needed 13 eConn neighbourhood(e) at layer lc 14 if eConn >Mmax // shrink connections of e // if lc \u00bc 0 thenMmax \u00bcMmax0 15 eNewConn SELECT-NEIGHBORS(e, eConn,Mmax, lc) // Algorithm 3 or Algorithm 4 16 set neighbourhood(e) at layer lc to eNewConn 17 ep W 18 if l > L 19 set enter-point for hnsw to q\nAlgorithm 2.", "K-NN-SEARCH(hnsw, q, K, ef)\nInput: multilayer graph hnsw, query element q, number of nearest neighbors to return K, size of the dynamic candidate list ef Output: K nearest elements to q 1 W ; // set for the current nearest elements 2 ep get enter-point for hnsw 3 L level of ep // top layer for hnsw 4 for lc L . . .", "https://github.com/nmslib/hnsw.", "https://github.com/nmslib/hnsw."]}
{"doc_id": 5,"title": "bart","abstract": [ "Recent work has shown gains by improving the distribution of\n1Code and pre-trained models for BART are available at https://github.com/pytorch/fairseq and https://huggingface.co/transformers\nmasked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).", "\nIn this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.", "BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.", "BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).", "\nBART is particularly effective when fine tuned for text generation but also works well for comprehension tasks.", "\nBART also opens up new ways of thinking about fine tuning.", "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.", "These layers are trained\nto essentially translate the foreign language to noised English, by propagation through BART, thereby using BART as a pre-trained target-side language model.", "We find that BART exhibits the most consistently strong performance across the full range of tasks we consider.", "\nBART is a denoising autoencoder that maps a corrupted document to the original document it was derived from."]}
{"doc_id": 6,"title": "bertscore","abstract": [ "\nIn this paper, we focus on sentence-level generation evaluation, and introduce BERTSCORE, an evaluation metric based on pre-trained BERT contextual embeddings (Devlin et al., 2019).", "BERTSCORE computes the similarity between two sentences as a weighted aggregation of cosine similarities between their tokens.", "\nBERTSCORE addresses three common pitfalls in n-gram based methods (Banerjee and Lavie, 2005).", "\nWe experiment with BERTSCORE on machine translation and image captioning tasks using multiple systems by correlating BERTSCORE and re-\nar X\niv :1\n90 4.", "Our experiments demonstrate that BERTSCORE correlates highly with human evaluations of the quality of machine translation and image captioning systems.", "In machine translation, BERTSCORE correlates better with segment-level human judgment than existing metrics on the common WMT17 benchmark (Bojar et al., 2017), including outperforming metrics learned specifically for this dataset.", "We also show that BERTSCORE is well correlated with human annotators for image captioning, surpassing SPICE, a popular taskspecific metric (Anderson et al., 2016), on the twelve 2015 COCO Captioning Challenge participating systems (Lin et al., 2014).", "Finally, we test the robustness of BERTSCORE on the adversarial paraphrase dataset PAWS (Zhang et al., 2019), and show that it is more robust to adversarial examples than other metrics.", "BERTSCORE is available at github.com/Tiiiger/bert score.", "In contrast to BLEU, BERTSCORE is not restricted to maximum n-gram length, but instead relies on contextualized embeddings that are able to capture dependencies of unbounded length."]}
{"doc_id": 7,"title": "roberta","abstract": [ "We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods.", "We call this configuration RoBERTa for Robustly optimized BERT approach.", "Specifically, RoBERTa is trained with dynamic masking (Section 4.1), FULL-SENTENCES without NSP loss (Section 4.2), large mini-batches (Section 4.3) and a larger byte-level BPE (Section 4.4).", "\nTo help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters).", "When controlling for training data, we observe that RoBERTa provides a large improvement over the originally reported BERTLARGE results, reaffirming the importance of the design choices we explored in Section 4.", "We train RoBERTa over the combined data with the same number of training steps as before (100K).", "We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K.", "\nIn the rest of the paper, we evaluate our best RoBERTa model on the three different benchmarks: GLUE, SQuaD and RACE.", "\nwe consider RoBERTa trained for 500K steps over all five of the datasets introduced in Section 3.2.", "In the first setting (single-task, dev) we finetune RoBERTa separately for each of the GLUE tasks, using only the training data for the corresponding task."]}
{"doc_id": 8,"title": "skip-gram model","abstract": ["The recently introduced continuous Skip-gram model is an effici nt method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.","\nRecently, Mikolov et al. [8] introduced the Skip-gram model, an efficient method for learning highquality vector representations of words from large amountsof unstructured text data.","\nIn this paper we present several extensions of the original Skip-gram model.","In addition, we present a simplified variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results in faster training and better vector representations for frequent words, compared to more complex hierarchical softmax that was used in the prior work [8].","Therefore, using vectors to represent the whole phrases makes the Skip-gram model considerably more expressive.","\nFinally, we describe another interesting property of the Skip-gram model.","\nThe training objective of the Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document.","More formally, given a sequence of training wordsw1, w2, w3, . . . , wT , the objective of the Skip-gram model is to maximize the averg log probability\n1\nT\nT \u2211\nt=1\n\u2211\n\u2212c\u2264j\u2264c,j 6=0\nlog p(wt+j |wt) (1)\nwherec is the size of the training context (which can be a function ofthe center wordwt).","For example, while the Skip-gram model benefits from observing the co-occurrences of \u201cFrance\u201d and \u201cParis\u201d, it benefits much less from observing the frequent co-o currences of \u201cFrance\u201d and \u201cthe\u201d, as nearly every word co-occurs frequently within a sentence with \u201cthe\u201d.","\nFor training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words)."]}
{"doc_id": 9,"title": "linformer","abstract": [ "\nIn Figure 2 (top right), we plot the inference speed of Linformer and standard Transformer versus sequence length, while holding the total number of tokens fixed.", "We see that while standard Transformer becomes slower at longer sequence lengths, the Linformer speed remains relatively flat and is significantly faster at long sequences.", "\nAdditional Efficiency Techniques Several additional techniques can be introduced on top of Linformer to further optimize for both performance and efficiency:\nParameter sharing between projections: One can share parameters for the linear projection matrices Ei, Fi across layers and heads.", "\nEffect of projected dimension: We experiment with various values for the projected dimension k. (We use the same k across all layers and heads of Linformer.) In the Figure 3(a) and (b), we plot the validation perplexity curves for both the standard Transformer and the Linformer across different k, for maximum sequence lengths n = 512 and n = 1024.", "As expected, the Linformer performs better as projected dimension k increases.", "However, even at k = 128 for n = 512 and k = 256 for n = 1024, Linformer\u2019s performance is already nearly on par with the original Transformer.", "Note that when we use just a single projection matrix (i.e. for layerwise sharing), the resulting Linformer model\u2019s validation perplexity almost matches that of the the non-shared model.", "\nEffect of longer sequences: We evaluate the effect of sequence length during Linformer pretraining.", "In the Figure 3(d), we plot the validation perplexity for Linformer with n \u2208 {512, 1024, 2048, 4096},\nholding projected dimension k fixed at 256.", "This further empirically supports our assertion that the Linformer is linear-time."]}
