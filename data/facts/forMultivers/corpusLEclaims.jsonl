{"doc_id": 0,"title": "sgc","abstract": ["We refer to this simplified linear model", "Simple Graph Convolution (SGC).", "\nIn contrast to its nonlinear counterparts", "the SGC is intuitively interpretable", "we provide a theoretical analysis from the graph convolution perspective.", "Notably, feature extraction in SGC corresponds to a single fixed filter applied to each feature dimension.", "\nThrough an empirical assessment on node classification benchmark datasets for citation and social networks", "we show that the SGC achieves comparable performance to GCN", "other state-of-the-art graph neural networks.", "Finally, we demonstrate that SGC extrapolates its effectiveness to a wide-range of downstream tasks.", "In particular, SGC rivals, if not surpasses", "GCN-based approaches on text classification, user geolocation, relation extraction,", "zero-shot image classification tasks.", "\nWe follow Kipf & Welling (2017) to introduce GCNs (", "subsequently SGC) in the context of node classification.", "We define the degree matrix D = diag(d1, . . .", "The resulting classifier becomes\nŶSGC"]}
{"doc_id": 1,"title": "shap","abstract": ["We then prove that results from game theory that guarantee a unique solution apply to the entire class of additive feature attribution methods (Section 3), and we propose SHAP values", "a unified measure of feature importance that other methods approximate (Section 4).", "We propose new SHAP value estimation methods,", "demonstrate they show better agreement with human intuition as measured by user studies", "better discriminate between model output classes", "existing methods (Section 5).", "\nClassic Shapley value estimation: There are three previous methods that use classic equations from cooperative game theory to compute explanations of model predictions: Shapley regression values [3]", "Shapley sampling values [8],", "Quantitative Input Influence [2].", "\nShapley regression values is a method designed to compute feature importance for linear models in the presence of multicollinearity.", "Then the prediction of the two models is compared on the current input fS∪{i}(xS∪{i})− fS(xS)", "where xS represents the values of the input features in the set S.", "The Shapley values are a weighted average of all possible differences:\n", "\nShapley sampling values are meant to explain any model by applying sampling approximations to Equation 4,", "by approximating the effect of removing a variable from the model by integrating over samples from the training data set.", "Since the explanation model form of Shapley sampling values is the same", "Shapley regression values it is also an additive feature attribution method.", "\nQuantitative Input Influence is a broader framework that addresses more than just feature attributions, but", "part of their method they independently proposed a sampling approximation to Shapley values that is nearly identical to Shapley sampling values."]}
{"doc_id": 2,"title": "cbow","abstract": ["(4) We denote this model further as CBOW,", "unlike standard bag-of-words model", "it uses continuous distributed representation of the context.", "\nThe second architecture is similar to CBOW,", "instead of predicting the current word based on the context", "it tries to maximize classification of a word based on another word in the same sentence.", "The results using the CBOW architecture with different choice of word vector dimensionality", "increasing amount of the training data are shown in Table 2.", "The CBOW architecture works better than the NNLM on the syntactic tasks,", "about the same on the semantic one.", "Finally, the Skip-gram architecture works slightly worse on the syntactic task than the CBOW model (but still better than the NNLM),", "much better on the semantic part of the test", "all the other models.", "The CBOW model was trained on subset\n", "Note that due to the overhead of the distributed framework", "the CPU usage of the CBOW model", "the Skip-gram model are much closer to each other", "their single-machine implementations.", "Using the DistBelief distributed framework, it should be possible to train the CBOW", "Skip-gram models even on corpora with one trillion words", "for basically unlimited size of the vocabulary."]}
{"doc_id": 3,"title": "xgboost","abstract": ["\nIn this paper, we describe XGBoost", "a scalable machine learning system for tree boosting.", "Among the 29 challenge winning solutions 3 published at Kaggle’s blog during 2015", "17 solutions used XGBoost.", "Among these solutions, eight solely used XGBoost to train the model,", "most others combined XGBoost with neural nets in ensembles.", "The success of the system was also witnessed in KDDCup 2015", "where XGBoost was used by every winning team in the top10.", "Moreover, the winning teams reported that ensemble methods outperform a well-configured XGBoost by only a small amount [1].", "While domain dependent data analysis and feature engineering play an important role in these solutions", "the fact that XGBoost is the consensus choice of learner shows the impact and importance of our system and tree boosting.", "\nThe most important factor behind the success of XGBoost is its scalability in all scenarios.", "The scalability of XGBoost is due to several important systems and algorithmic optimizations.", "More importantly, XGBoost exploits out-of-core\n2https://github.com/dmlc/xgboost 3Solutions", "Most existing single machine tree boosting implementations, such", "scikit-learn [20]", "R’s gbm [21] as well", "the single machine version of XGBoost support the exact greedy algorithm."]}
{"doc_id": 4,"title": "hnsw","abstract": ["\nIn this paper we propose the Hierarchical Navigable Small World (Hierarchical NSW, HNSW),", "INSERT(hnsw; q, M,Mmax, efConstruction", "mL)\nInput: multilayer graph hnsw, new element q", "number of established connections M", "maximum number of connections for each element per layer Mmax", "size of the dynamic candidate list efConstruction", "normalization factor for level generationmL Output: update hnsw inserting element q 1 W", "== list for the currently found nearest elements 2 ep get enter-point for hnsw 3 L level of ep // top layer for hnsw 4", "K-NN-SEARCH(hnsw, q, K, ef)\n"]}
{"doc_id": 5,"title": "bart","abstract": ["Recent work has shown gains by improving the distribution of\n1Code", "pre-trained models for BART are available at https://github.com/pytorch/fairseq and", "\nIn this paper, we present BART", "which pre-trains a model combining Bidirectional", "Auto-Regressive Transformers.", "BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.", "BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen", "generalizing BERT (due to the bidirectional encoder)", "GPT (with the left-to-right decoder),", "many other more recent pretraining schemes (see Figure 1).", "\nBART is particularly effective when fine tuned for text generation", "also works well for comprehension tasks.", "\nBART also opens up new ways of thinking about fine tuning.", "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.", "These layers are trained\nto essentially translate the foreign language to noised English, by propagation through BART, thereby using BART", "a pre-trained target-side language model.", "We find that BART exhibits the most consistently strong performance across the full range of tasks we consider.", "\nBART is a denoising autoencoder that maps a corrupted document to the original document it was derived from."]}
{"doc_id": 6,"title": "bertscore","abstract": ["\nIn this paper, we focus on sentence-level generation evaluation, and introduce BERTSCORE", "an evaluation metric based on pre-trained BERT contextual embeddings (Devlin et al., 2019).", "BERTSCORE computes the similarity between two sentences", "a weighted aggregation of cosine similarities between their tokens.", "\nBERTSCORE addresses three common pitfalls in n-gram based methods (Banerjee and Lavie, 2005).", "\nWe experiment with BERTSCORE on machine translation", "image captioning tasks using multiple systems by correlating BERTSCORE and re-\n", "Our experiments demonstrate that BERTSCORE correlates highly with human evaluations of the quality of machine translation and image captioning systems.", "In machine translation, BERTSCORE correlates better with segment-level human judgment", "existing metrics on the common WMT17 benchmark (Bojar et al., 2017)", "including outperforming metrics learned specifically for this dataset.", "We also show that BERTSCORE is well correlated with human annotators for image captioning, surpassing SPICE", "a popular taskspecific metric (Anderson et al., 2016)", "on the twelve 2015 COCO Captioning Challenge participating systems (Lin et al., 2014).", "Finally, we test the robustness of BERTSCORE on the adversarial paraphrase dataset PAWS", "BERTSCORE is available at github.com/Tiiiger/bert score.", "In contrast to BLEU, BERTSCORE is not restricted to maximum n-gram length,", "instead relies on contextualized embeddings that are able to capture dependencies of unbounded length."]}
{"doc_id": 7,"title": "roberta","abstract": ["We find that BERT was significantly undertrained", "propose an improved recipe for training BERT models, which we call RoBERTa, that can match", "exceed the performance of all of the post-BERT methods.", "We call this configuration RoBERTa for Robustly optimized BERT approach.", "Specifically, RoBERTa is trained with dynamic masking (Section 4.1)", "FULL-SENTENCES without NSP loss (Section 4.2)", "large mini-batches (Section 4.3)", "a larger byte-level BPE (Section 4.4).", "\nTo help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective)", "we begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16", "355M parameters).", "When controlling for training data, we observe that RoBERTa provides a large improvement over the originally reported BERTLARGE results", "reaffirming the importance of the design choices we explored in Section 4.", "We train RoBERTa over the combined data with the same number of training steps", "before (100K).", "We observe further improvements in performance across all downstream tasks", "validating the importance of data size and diversity in pretraining.9\n", "\nIn the rest of the paper", "we evaluate our best RoBERTa model on the three different benchmarks: GLUE, SQuaD and RACE.", "\nwe consider RoBERTa trained for 500K steps over all five of the datasets introduced in Section 3.2.", "In the first setting (single-task", "dev) we finetune RoBERTa separately for each of the GLUE tasks", "using only the training data for the corresponding task."]}
{"doc_id": 8,"title": "skip-gram model","abstract": ["The recently introduced continuous Skip-gram model is an effici nt method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.", "\nRecently, Mikolov et al.", "\nIn this paper we present several extensions of the original Skip-gram model.", "In addition, we present a simplified variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results in faster training", "better vector representations for frequent words", "compared to more complex hierarchical softmax that was used in the prior work [8].", "Therefore, using vectors to represent the whole phrases makes the Skip-gram model considerably more expressive.", "\nFinally, we describe another interesting property of the Skip-gram model.", "\nThe training objective of the Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document.", "More formally, given a sequence of training wordsw1, w2, w3, . . .", "For example, while the Skip-gram model benefits from observing the co-occurrences of “France” and “Paris”", "it benefits much less from observing the frequent co-o currences of “France” and “the”,", "nearly every word co-occurs frequently within a sentence with “the”.", "\nFor training the Skip-gram models", "we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words)."]}
{"doc_id": 9,"title": "linformer","abstract": ["\nIn Figure 2 (top right)", "we plot the inference speed of Linformer", "standard Transformer versus sequence length,", "holding the total number of tokens fixed.", "We see that while standard Transformer becomes slower at longer sequence lengths", "the Linformer speed remains relatively flat", "is significantly faster at long sequences.", "\nEffect of projected dimension: We experiment with various values for the projected dimension", "As expected, the Linformer performs better", "projected dimension k increases.", "However, even at k = 128 for n = 512", "Note that when we use just a single projection matrix (i.e. for layerwise sharing)", "the resulting Linformer model’s validation perplexity almost matches that of the the non-shared model.", "\nEffect of longer sequences: We evaluate the effect of sequence length during Linformer pretraining.", "In the Figure 3(d), we plot the validation perplexity for Linformer with n ∈ {512, 1024, 2048, 4096}", "\nholding projected dimension k fixed at 256.", "This further empirically supports our assertion that the Linformer is linear-time."]}
